{"cells":[{"cell_type":"code","source":["!java -version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dff34ff-1961-4769-ba8a-9d63be4a0e2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">openjdk version &#34;1.8.0_275&#34;\r\nOpenJDK Runtime Environment (Zulu 8.50.0.51-CA-linux64) (build 1.8.0_275-b01)\r\nOpenJDK 64-Bit Server VM (Zulu 8.50.0.51-CA-linux64) (build 25.275-b01, mixed mode)\r\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">openjdk version &#34;1.8.0_275&#34;\r\nOpenJDK Runtime Environment (Zulu 8.50.0.51-CA-linux64) (build 1.8.0_275-b01)\r\nOpenJDK 64-Bit Server VM (Zulu 8.50.0.51-CA-linux64) (build 25.275-b01, mixed mode)\r\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sc.version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"129af4d4-0a5b-40bc-a4a7-92f623441ecc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: &#39;3.1.0&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: &#39;3.1.0&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#!pip install mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6351029a-f885-4ea0-805d-2b7eba18c603"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#import math\n#import pandas as pd\n#import matplotlib.pylab as plt\n#%matplotlib inline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb339128-b151-4cc3-a414-ccda5baa0ad3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# carregar tabela de dados\ndf = table(\"cadastro_nomes_sexo\")\n#df = spark.sql(\"select * from cadastro_nomes_sexo\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"098e35dc-3fff-4e5a-a03f-bacec49db898"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"771c6b06-daed-4c82-9124-3942dcb3ab13"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- PNOME: string (nullable = true)\n |-- SEXO: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- PNOME: string (nullable = true)\n-- SEXO: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"740d1007-0d7a-4516-9312-883a1a240716"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+----+\n|    PNOME|SEXO|\n+---------+----+\n|    MARIA|   F|\n|     JOSE|   M|\n|      ANA|   F|\n|     JOAO|   M|\n|   CARLOS|   M|\n|    PAULO|   M|\n|  ANTONIO|   M|\n|     LUIZ|   M|\n|FRANCISCO|   M|\n|  MARCELO|   M|\n+---------+----+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----+\n    PNOME|SEXO|\n+---------+----+\n    MARIA|   F|\n     JOSE|   M|\n      ANA|   F|\n     JOAO|   M|\n   CARLOS|   M|\n    PAULO|   M|\n  ANTONIO|   M|\n     LUIZ|   M|\nFRANCISCO|   M|\n  MARCELO|   M|\n+---------+----+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd = df.rdd.filter(lambda x: x['SEXO'] in ['M', 'F'])\n#rdd = df.rdd\n#rdd = df.rdd.map(list)\n#rdd = df.rdd.map(tuple)\nrdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a4da81-02ef-4213-a70a-c9e55476cf75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: PythonRDD[9] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: PythonRDD[9] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["type(rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a441b331-92f3-47a8-bbf4-dbb62bf97c66"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: pyspark.rdd.PipelinedRDD</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: pyspark.rdd.PipelinedRDD</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51e0d99e-1acd-4dbc-baea-5b46e735b5c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: [Row(PNOME=&#39;MARIA&#39;, SEXO=&#39;F&#39;),\n Row(PNOME=&#39;JOSE&#39;, SEXO=&#39;M&#39;),\n Row(PNOME=&#39;ANA&#39;, SEXO=&#39;F&#39;),\n Row(PNOME=&#39;JOAO&#39;, SEXO=&#39;M&#39;),\n Row(PNOME=&#39;CARLOS&#39;, SEXO=&#39;M&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: [Row(PNOME=&#39;MARIA&#39;, SEXO=&#39;F&#39;),\n Row(PNOME=&#39;JOSE&#39;, SEXO=&#39;M&#39;),\n Row(PNOME=&#39;ANA&#39;, SEXO=&#39;F&#39;),\n Row(PNOME=&#39;JOAO&#39;, SEXO=&#39;M&#39;),\n Row(PNOME=&#39;CARLOS&#39;, SEXO=&#39;M&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\n\nrdd2 = rdd.map(lambda x: Row(\n  nome=x['PNOME'],\n  sexo={'F': 0, 'M': 1, 'X': 9}[x['SEXO']]\n))\nrdd2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4b837d2-11cd-4d58-a8a4-34e90490e238"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[10]: PythonRDD[11] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: PythonRDD[11] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd2.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"258cd10b-7b6e-4bb3-9e20-a7e5c2d35420"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: [Row(nome=&#39;MARIA&#39;, sexo=0),\n Row(nome=&#39;JOSE&#39;, sexo=1),\n Row(nome=&#39;ANA&#39;, sexo=0),\n Row(nome=&#39;JOAO&#39;, sexo=1),\n Row(nome=&#39;CARLOS&#39;, sexo=1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: [Row(nome=&#39;MARIA&#39;, sexo=0),\n Row(nome=&#39;JOSE&#39;, sexo=1),\n Row(nome=&#39;ANA&#39;, sexo=0),\n Row(nome=&#39;JOAO&#39;, sexo=1),\n Row(nome=&#39;CARLOS&#39;, sexo=1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["MAIOR_QTDE_LETRAS = 16 # fixado para o maior arquivo\n\ndef incluir_letras(row):\n  row_dict = row.asDict()\n  rev = row_dict['nome'][::-1]\n  tam = len(rev)\n  letras = [0] * MAIOR_QTDE_LETRAS\n  \n  for i in range(tam):\n    letras[i] = ord(rev[i]) - 64\n  row_dict['letras'] = letras\n  \n  new_row = Row(**row_dict)\n  return new_row"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd5ccac6-f7ec-4c7d-8f0e-018d11b1e0ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3 = rdd2.map(incluir_letras)\nrdd3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46daac5b-32b7-465b-a1e3-94a8b6e8ecf3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[13]: PythonRDD[13] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: PythonRDD[13] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ceb1545d-cf96-4418-a786-110968e34cbd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: [Row(nome=&#39;MARIA&#39;, sexo=0, letras=[1, 9, 18, 1, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;JOSE&#39;, sexo=1, letras=[5, 19, 15, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;ANA&#39;, sexo=0, letras=[1, 14, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;JOAO&#39;, sexo=1, letras=[15, 1, 15, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;CARLOS&#39;, sexo=1, letras=[19, 15, 12, 18, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [Row(nome=&#39;MARIA&#39;, sexo=0, letras=[1, 9, 18, 1, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;JOSE&#39;, sexo=1, letras=[5, 19, 15, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;ANA&#39;, sexo=0, letras=[1, 14, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;JOAO&#39;, sexo=1, letras=[15, 1, 15, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n Row(nome=&#39;CARLOS&#39;, sexo=1, letras=[19, 15, 12, 18, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df3 = rdd3.toDF()\ndf3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a065e561-4e43-4d4c-8617-9a3c672cb5f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[15]: DataFrame[nome: string, sexo: bigint, letras: array&lt;bigint&gt;]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: DataFrame[nome: string, sexo: bigint, letras: array&lt;bigint&gt;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df3.select(\"nome\", \"sexo\").describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03bd013a-937f-4dc7-9b33-abe992fe7aa5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+------+------------------+\n|summary|  nome|              sexo|\n+-------+------+------------------+\n|  count| 41150|             41150|\n|   mean|   NaN|0.4680194410692588|\n| stddev|  null|0.4989822586799242|\n|    min| AARAO|                 0|\n|    max|ZYSMAN|                 1|\n+-------+------+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------+------------------+\nsummary|  nome|              sexo|\n+-------+------+------------------+\n  count| 41150|             41150|\n   mean|   NaN|0.4680194410692588|\n stddev|  null|0.4989822586799242|\n    min| AARAO|                 0|\n    max|ZYSMAN|                 1|\n+-------+------+------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n\ndef converter_para_ponto_rotulado(row):\n    obj = (row[\"nome\"], row[\"sexo\"], Vectors.dense(row[\"letras\"]))\n    return obj"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6429ddf6-7e59-4647-a8f7-de2572052e79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd4 = rdd3.map(converter_para_ponto_rotulado)\nrdd4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4582b9c4-6496-49f5-93c6-f4a38018ada7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: PythonRDD[26] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: PythonRDD[26] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd4.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ffb4a73-0ca4-4ee4-9acd-e774ae6088b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[19]: [(&#39;MARIA&#39;,\n  0,\n  DenseVector([1.0, 9.0, 18.0, 1.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;JOSE&#39;,\n  1,\n  DenseVector([5.0, 19.0, 15.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;ANA&#39;,\n  0,\n  DenseVector([1.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;JOAO&#39;,\n  1,\n  DenseVector([15.0, 1.0, 15.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;CARLOS&#39;,\n  1,\n  DenseVector([19.0, 15.0, 12.0, 18.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: [(&#39;MARIA&#39;,\n  0,\n  DenseVector([1.0, 9.0, 18.0, 1.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;JOSE&#39;,\n  1,\n  DenseVector([5.0, 19.0, 15.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;ANA&#39;,\n  0,\n  DenseVector([1.0, 14.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;JOAO&#39;,\n  1,\n  DenseVector([15.0, 1.0, 15.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n (&#39;CARLOS&#39;,\n  1,\n  DenseVector([19.0, 15.0, 12.0, 18.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df4 = rdd4.toDF([\"nome\", \"real\", \"features\"])\ndf4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c623c57f-44fc-49c7-b66b-60f704aa5653"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[20]: DataFrame[nome: string, real: bigint, features: vector]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: DataFrame[nome: string, real: bigint, features: vector]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df4.show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"def3279e-89ef-43b9-99fa-d75074c8e1cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------+----+---------------------------------------------------------------------+\n|nome  |real|features                                                             |\n+------+----+---------------------------------------------------------------------+\n|MARIA |0   |[1.0,9.0,18.0,1.0,13.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]  |\n|JOSE  |1   |[5.0,19.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\n|ANA   |0   |[1.0,14.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\n|JOAO  |1   |[15.0,1.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\n|CARLOS|1   |[19.0,15.0,12.0,18.0,1.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]|\n+------+----+---------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+---------------------------------------------------------------------+\nnome  |real|features                                                             |\n+------+----+---------------------------------------------------------------------+\nMARIA |0   |[1.0,9.0,18.0,1.0,13.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]  |\nJOSE  |1   |[5.0,19.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\nANA   |0   |[1.0,14.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\nJOAO  |1   |[15.0,1.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\nCARLOS|1   |[19.0,15.0,12.0,18.0,1.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]|\n+------+----+---------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import PCA\n\npca = PCA(\n  k = 10,\n  inputCol = \"features\",\n  outputCol = \"pcaFeatures\"\n)\n\n#pcaModel = pca.fit(df4)\n#pcaResult = pcaModel.transform(df4).select(\"nome\", \"real\", \"pcaFeatures\")\n#pcaResult.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0db7f2fc-734d-40ee-be05-cc87c9def90c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# usar redução de dimensionalidade PCA\n#df5 = pcaResult.withColumnRenamed(\"pcaFeatures\", \"features\")\n\n# não usar PCA\ndf5 = df4\n\ndf5.show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6f4f72c-d890-4cf8-8194-174f39d44261"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------+----+---------------------------------------------------------------------+\n|nome  |real|features                                                             |\n+------+----+---------------------------------------------------------------------+\n|MARIA |0   |[1.0,9.0,18.0,1.0,13.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]  |\n|JOSE  |1   |[5.0,19.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\n|ANA   |0   |[1.0,14.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\n|JOAO  |1   |[15.0,1.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\n|CARLOS|1   |[19.0,15.0,12.0,18.0,1.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]|\n+------+----+---------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+---------------------------------------------------------------------+\nnome  |real|features                                                             |\n+------+----+---------------------------------------------------------------------+\nMARIA |0   |[1.0,9.0,18.0,1.0,13.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]  |\nJOSE  |1   |[5.0,19.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\nANA   |0   |[1.0,14.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]   |\nJOAO  |1   |[15.0,1.0,15.0,10.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] |\nCARLOS|1   |[19.0,15.0,12.0,18.0,1.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]|\n+------+----+---------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# dividir dados entre treino e teste\n(dados_treino, dados_teste) = df5.randomSplit([0.7, 0.3], seed=42)\n\ndados_treino.cache()\ndados_teste.cache()\n\nprint(\"dados de treino:\", dados_treino.count())\nprint(\"dados de teste: \", dados_teste.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff96f724-66ab-4e1f-bc44-69440947ce86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">dados de treino: 28895\ndados de teste:  12255\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">dados de treino: 28895\ndados de teste:  12255\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# https://spark.apache.org/docs/latest/ml-statistics.html\n\n'''from pyspark.ml.stat import Correlation\n\nr1 = Correlation.corr(dados_treino, \"features\").head()\nprint(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n\nr2 = Correlation.corr(dados_treino, \"features\", \"spearman\").head()\nprint(\"Spearman correlation matrix:\\n\" + str(r2[0]))'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5df7c79a-ee95-4339-8e73-865f3d71606a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[117]: &#39;from pyspark.ml.stat import Correlation\\n\\nr1 = Correlation.corr(dados_treino, &#34;features&#34;).head()\\nprint(&#34;Pearson correlation matrix:\\n&#34; + str(r1[0]))\\n\\nr2 = Correlation.corr(dados_treino, &#34;features&#34;, &#34;spearman&#34;).head()\\nprint(&#34;Spearman correlation matrix:\\n&#34; + str(r2[0]))&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[117]: &#39;from pyspark.ml.stat import Correlation\\n\\nr1 = Correlation.corr(dados_treino, &#34;features&#34;).head()\\nprint(&#34;Pearson correlation matrix:\\n&#34; + str(r1[0]))\\n\\nr2 = Correlation.corr(dados_treino, &#34;features&#34;, &#34;spearman&#34;).head()\\nprint(&#34;Spearman correlation matrix:\\n&#34; + str(r2[0]))&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.stat import ChiSquareTest\n\nr = ChiSquareTest.test(dados_treino, \"features\", \"real\").head()\nprint(\"pValues: \" + str(r.pValues))\nprint(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\nprint(\"statistics: \" + str(r.statistics))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c88e5292-3423-4ce5-8804-2104e4f3d3ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">pValues: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.637257105561844e-09,1.6843575534775113e-05,3.718938691055218e-05,0.03739793711488393,0.46830207592608697,0.32302180098266264,0.28777611678969917]\ndegreesOfFreedom: [20, 24, 26, 26, 26, 26, 26, 26, 26, 25, 25, 21, 16, 8, 2, 1]\nstatistics: [24932.73166384896,12661.439967337154,6324.827141049572,1186.5813205769834,713.8037043122233,733.1003573653618,314.1307721639996,222.472181892376,150.43242457758964,88.18921402675996,65.62013938285571,56.899226372573416,27.384930080638828,7.65080208702353,2.260070925469649,1.1299963530420603]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">pValues: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.637257105561844e-09,1.6843575534775113e-05,3.718938691055218e-05,0.03739793711488393,0.46830207592608697,0.32302180098266264,0.28777611678969917]\ndegreesOfFreedom: [20, 24, 26, 26, 26, 26, 26, 26, 26, 25, 25, 21, 16, 8, 2, 1]\nstatistics: [24932.73166384896,12661.439967337154,6324.827141049572,1186.5813205769834,713.8037043122233,733.1003573653618,314.1307721639996,222.472181892376,150.43242457758964,88.18921402675996,65.62013938285571,56.899226372573416,27.384930080638828,7.65080208702353,2.260070925469649,1.1299963530420603]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["'''from pyspark.ml.stat import Summarizer\n\n# create summarizer for multiple metrics \"mean\" and \"count\"\nsummarizer = Summarizer.metrics(\"mean\", \"count\")\n\n# compute statistics for multiple metrics with weight\ndados_treino.select(summarizer.summary(dados_treino.features, dados_treino.real)).show(truncate=False)\n\n# compute statistics for multiple metrics without weight\ndados_treino.select(summarizer.summary(dados_treino.features)).show(truncate=False)\n\n# compute statistics for single metric \"mean\" with weight\ndados_treino.select(Summarizer.mean(dados_treino.features, dados_treino.real)).show(truncate=False)\n\n# compute statistics for single metric \"mean\" without weight\ndados_treino.select(Summarizer.mean(dados_treino.features)).show(truncate=False)'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c055fcbc-f3b1-4429-94d9-39f614000560"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[116]: &#39;from pyspark.ml.stat import Summarizer\\n\\n# create summarizer for multiple metrics &#34;mean&#34; and &#34;count&#34;\\nsummarizer = Summarizer.metrics(&#34;mean&#34;, &#34;count&#34;)\\n\\n# compute statistics for multiple metrics with weight\\ndados_treino.select(summarizer.summary(dados_treino.features, dados_treino.real)).show(truncate=False)\\n\\n# compute statistics for multiple metrics without weight\\ndados_treino.select(summarizer.summary(dados_treino.features)).show(truncate=False)\\n\\n# compute statistics for single metric &#34;mean&#34; with weight\\ndados_treino.select(Summarizer.mean(dados_treino.features, dados_treino.real)).show(truncate=False)\\n\\n# compute statistics for single metric &#34;mean&#34; without weight\\ndados_treino.select(Summarizer.mean(dados_treino.features)).show(truncate=False)&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[116]: &#39;from pyspark.ml.stat import Summarizer\\n\\n# create summarizer for multiple metrics &#34;mean&#34; and &#34;count&#34;\\nsummarizer = Summarizer.metrics(&#34;mean&#34;, &#34;count&#34;)\\n\\n# compute statistics for multiple metrics with weight\\ndados_treino.select(summarizer.summary(dados_treino.features, dados_treino.real)).show(truncate=False)\\n\\n# compute statistics for multiple metrics without weight\\ndados_treino.select(summarizer.summary(dados_treino.features)).show(truncate=False)\\n\\n# compute statistics for single metric &#34;mean&#34; with weight\\ndados_treino.select(Summarizer.mean(dados_treino.features, dados_treino.real)).show(truncate=False)\\n\\n# compute statistics for single metric &#34;mean&#34; without weight\\ndados_treino.select(Summarizer.mean(dados_treino.features)).show(truncate=False)&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8220461b-1261-4776-ae81-7e2ca2021c98"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["from datetime import datetime\n\nmodels = {}\n\ndef evaluate_model(name, classifier, evaluator, train_data, test_data):\n\n  start = datetime.now()\n  model = classifier.fit(train_data)\n  end = datetime.now()\n  elapsed = int((end - start).total_seconds() * 1000)\n\n  predictions = model.transform(test_data)\n  score = evaluator.evaluate(predictions) * 100\n\n  models[name] = (model, score, elapsed)\n  print(model, '\\nScore: %.2f [%5s ms]' % (score, elapsed))\n  return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05cf531b-ee35-4a38-940f-f0df8590ae3c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# https://spark.apache.org/docs/latest/ml-tuning.html\nimport numpy as np\nimport re\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nNUMBER_KFOLD_SPLITS = 5 # number of splits in cross-validation\n\n# faz o ajuste fino do modelo, calculando os melhores hiperparâmetros\ndef fine_tune_model(estimator, params, evaluator, train_data, test_data):\n\n  print('\\nFine Tuning Model:')\n  print(estimator) #, \"\\nparams:\", params)\n  \n  crossval = CrossValidator(estimator=estimator,\n                            estimatorParamMaps=params,\n                            evaluator=evaluator,\n                            numFolds=NUMBER_KFOLD_SPLITS,\n                            seed=42,\n                            parallelism=3)\n  search = crossval.fit(train_data)\n\n  predictions = search.transform(test_data)\n  score = evaluator.evaluate(predictions) * 100\n\n  hyper_list = []\n  hyperparams = search.getEstimatorParamMaps()[np.argmax(search.avgMetrics)]\n  for i in range(len(hyperparams.items())):\n    hyper_name = re.search(\"name='(.+?)'\", str([x for x in hyperparams.items()][i])).group(1)\n    hyper_value = [x for x in hyperparams.items()][i][1]\n    hyper_list.append({hyper_name: hyper_value})\n\n  print('\\nBest Score: %.2f' % score)\n  print('Best Params:', hyper_list)\n  return search"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4874bf13-13b5-499b-ae05-a8d0b4d5e662"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["'''from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\navaliador = MulticlassClassificationEvaluator(\n    predictionCol = \"previsto\",\n    labelCol = \"real\",\n    metricName = \"accuracy\"\n)'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b48de5a8-95e6-428b-9b36-679dc8435c72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[77]: &#39;from pyspark.ml.evaluation import MulticlassClassificationEvaluator\\n\\navaliador = MulticlassClassificationEvaluator(\\n    predictionCol = &#34;previsto&#34;,\\n    labelCol = &#34;real&#34;,\\n    metricName = &#34;accuracy&#34;\\n)&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[77]: &#39;from pyspark.ml.evaluation import MulticlassClassificationEvaluator\\n\\navaliador = MulticlassClassificationEvaluator(\\n    predictionCol = &#34;previsto&#34;,\\n    labelCol = &#34;real&#34;,\\n    metricName = &#34;accuracy&#34;\\n)&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\navaliador = BinaryClassificationEvaluator(\n    rawPredictionCol = \"previsto\",\n    labelCol = \"real\",\n    metricName = \"areaUnderROC\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bea301e8-ef1c-4922-b0ad-7da09cc1c178"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\n\nestimador = LogisticRegression(\n  regParam=0.1,\n  maxIter=10,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.regParam, [1.0, 0.1, 0.01]) \\\n    .addGrid(estimador.maxIter, [10, 100, 1000]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\nevaluate_model('LR', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a6ee2a4-fea9-4d7f-a254-b751c4c62106"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">LogisticRegressionModel: uid=LogisticRegression_13082fe0fd69, numClasses=2, numFeatures=16 \nScore: 94.95 [ 1669 ms]\nOut[81]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">LogisticRegressionModel: uid=LogisticRegression_13082fe0fd69, numClasses=2, numFeatures=16 \nScore: 94.95 [ 1669 ms]\nOut[81]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Decision Tree\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\nestimador = DecisionTreeClassifier(\n  maxDepth=9,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.maxDepth, [2, 5, 7, 9, 11, 13]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\nevaluate_model('DT', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"673793c7-2d32-425d-9259-b8c5833075e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">DecisionTreeClassificationModel: uid=DecisionTreeClassifier_fcc58529b73c, depth=9, numNodes=199, numClasses=2, numFeatures=16 \nScore: 99.62 [ 2393 ms]\nOut[82]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DecisionTreeClassificationModel: uid=DecisionTreeClassifier_fcc58529b73c, depth=9, numNodes=199, numClasses=2, numFeatures=16 \nScore: 99.62 [ 2393 ms]\nOut[82]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Random Forest\nfrom pyspark.ml.classification import RandomForestClassifier\n\nestimador = RandomForestClassifier(\n  maxDepth=19,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.maxDepth, [9, 11, 13, 17, 19]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\nevaluate_model('RF', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3fe8ae8-c54b-47f7-9cf7-ef837cae9579"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">RandomForestClassificationModel: uid=RandomForestClassifier_3e168ff86bca, numTrees=20, numClasses=2, numFeatures=16 \nScore: 99.19 [22776 ms]\nOut[83]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">RandomForestClassificationModel: uid=RandomForestClassifier_3e168ff86bca, numTrees=20, numClasses=2, numFeatures=16 \nScore: 99.19 [22776 ms]\nOut[83]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Gradient-Boosted Trees (GBTs)\nfrom pyspark.ml.classification import GBTClassifier\n\nestimador = GBTClassifier(\n  #maxDepth=10,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.maxDepth, [5, 10]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\n# só classificação binária!\nevaluate_model('GB', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa9f3dc-c599-4db5-986d-de0edf9b5f31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">GBTClassificationModel: uid = GBTClassifier_b281d49decd1, numTrees=20, numClasses=2, numFeatures=16 \nScore: 98.93 [18598 ms]\nOut[103]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">GBTClassificationModel: uid = GBTClassifier_b281d49decd1, numTrees=20, numClasses=2, numFeatures=16 \nScore: 98.93 [18598 ms]\nOut[103]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Multilayer Perceptron (MLP)\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\n\nestimador = MultilayerPerceptronClassifier(\n  #maxIter=100,\n  layers=[4, 5, 3],\n  #blockSize=128,\n  seed=42,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.layers, [[4,5,3], [4,8,3], [4,5,6,3]]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\n# está dando erro...\n#evaluate_model('MLP', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"651ab842-808e-49b1-a323-8a65540668a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3112302689744671&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span> <span class=\"ansi-red-fg\"># está dando erro...</span>\n<span class=\"ansi-green-fg\">---&gt; 20</span><span class=\"ansi-red-fg\"> </span>evaluate_model<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;MLP&#39;</span><span class=\"ansi-blue-fg\">,</span> estimador<span class=\"ansi-blue-fg\">,</span> avaliador<span class=\"ansi-blue-fg\">,</span> dados_treino<span class=\"ansi-blue-fg\">,</span> dados_teste<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3112302689744656&gt;</span> in <span class=\"ansi-cyan-fg\">evaluate_model</span><span class=\"ansi-blue-fg\">(name, classifier, evaluator, train_data, test_data)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span>   predictions <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>test_data<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\">   </span>score <span class=\"ansi-blue-fg\">=</span> evaluator<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>predictions<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">100</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span>   models<span class=\"ansi-blue-fg\">[</span>name<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">,</span> score<span class=\"ansi-blue-fg\">,</span> elapsed<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">evaluate</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     82</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     83</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 84</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     85</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     86</span>             <span class=\"ansi-green-fg\">raise</span> ValueError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Params must be a param map but got %s.&#34;</span> <span class=\"ansi-blue-fg\">%</span> type<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">_evaluate</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 120</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>     <span class=\"ansi-green-fg\">def</span> isLargerBetter<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o98134.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46337.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46337.0 (TID 44667) (ip-10-172-254-191.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function(ProbabilisticClassificationModel$$Lambda$6257/1836602722: (struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;) =&gt; struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: A &amp; B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:123)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2711)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2658)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2848)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2439)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2460)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2504)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1258)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:197)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:178)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:180)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:180)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:272)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:103)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:123)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:102)\n\tat sun.reflect.GeneratedMethodAccessor335.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(ProbabilisticClassificationModel$$Lambda$6257/1836602722: (struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;) =&gt; struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: A &amp; B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:123)\n\t... 20 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46337.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46337.0 (TID 44667) (ip-10-172-254-191.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function(ProbabilisticClassificationModel$$Lambda$6257/1836602722: (struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;) =&gt; struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;)","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3112302689744671&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span> <span class=\"ansi-red-fg\"># está dando erro...</span>\n<span class=\"ansi-green-fg\">---&gt; 20</span><span class=\"ansi-red-fg\"> </span>evaluate_model<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;MLP&#39;</span><span class=\"ansi-blue-fg\">,</span> estimador<span class=\"ansi-blue-fg\">,</span> avaliador<span class=\"ansi-blue-fg\">,</span> dados_treino<span class=\"ansi-blue-fg\">,</span> dados_teste<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3112302689744656&gt;</span> in <span class=\"ansi-cyan-fg\">evaluate_model</span><span class=\"ansi-blue-fg\">(name, classifier, evaluator, train_data, test_data)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span>   predictions <span class=\"ansi-blue-fg\">=</span> model<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>test_data<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\">   </span>score <span class=\"ansi-blue-fg\">=</span> evaluator<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>predictions<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">*</span> <span class=\"ansi-cyan-fg\">100</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span>   models<span class=\"ansi-blue-fg\">[</span>name<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">,</span> score<span class=\"ansi-blue-fg\">,</span> elapsed<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">evaluate</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     82</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     83</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 84</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     85</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     86</span>             <span class=\"ansi-green-fg\">raise</span> ValueError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Params must be a param map but got %s.&#34;</span> <span class=\"ansi-blue-fg\">%</span> type<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">_evaluate</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 120</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>     <span class=\"ansi-green-fg\">def</span> isLargerBetter<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    108</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    109</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 110</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    111</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    112</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o98134.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46337.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46337.0 (TID 44667) (ip-10-172-254-191.us-west-2.compute.internal executor driver): org.apache.spark.SparkException: Failed to execute user defined function(ProbabilisticClassificationModel$$Lambda$6257/1836602722: (struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;) =&gt; struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: A &amp; B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:123)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2711)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2658)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2652)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2652)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1255)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1255)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2919)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2848)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2439)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2460)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2479)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2504)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1258)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:197)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:178)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:180)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:180)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:272)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:103)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:123)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:102)\n\tat sun.reflect.GeneratedMethodAccessor335.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(ProbabilisticClassificationModel$$Lambda$6257/1836602722: (struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;) =&gt; struct&lt;type:tinyint,size:int,indices:array&lt;int&gt;,values:array&lt;double&gt;&gt;)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:754)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:192)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:119)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:732)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:735)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: A &amp; B Dimension mismatch!\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.ann.BreezeUtil$.dgemm(BreezeUtil.scala:41)\n\tat org.apache.spark.ml.ann.AffineLayerModel.eval(Layer.scala:164)\n\tat org.apache.spark.ml.ann.FeedForwardModel.forward(Layer.scala:508)\n\tat org.apache.spark.ml.ann.FeedForwardModel.predictRaw(Layer.scala:561)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:332)\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.predictRaw(MultilayerPerceptronClassifier.scala:274)\n\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel.$anonfun$transform$2(ProbabilisticClassifier.scala:123)\n\t... 20 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Linear Support Vector Machine (SVM)\nfrom pyspark.ml.classification import LinearSVC\n\nestimador = LinearSVC(\n  maxIter=100,\n  regParam=0.1,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.maxIter, [10, 50, 100, 500]) \\\n    .addGrid(estimador.regParam, [0.05, 0.1, 0.5]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\n# só classificação binária!\nevaluate_model('LSVM', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"295690ad-e8c9-4a57-b70b-77f1adee3378"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">LinearSVCModel: uid=LinearSVC_6fc815743a5f, numClasses=2, numFeatures=16 \nScore: 95.28 [ 9664 ms]\nOut[84]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">LinearSVCModel: uid=LinearSVC_6fc815743a5f, numClasses=2, numFeatures=16 \nScore: 95.28 [ 9664 ms]\nOut[84]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Naïve Bayes\nfrom pyspark.ml.classification import NaiveBayes\n\nestimador = NaiveBayes(\n  smoothing=0.25,\n  modelType=\"multinomial\",\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.smoothing, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n    .addGrid(estimador.modelType, [\"multinomial\"]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\nevaluate_model('NB', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a1ed32c-a3c7-4651-af68-5f70705dc3a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">NaiveBayesModel: uid=NaiveBayes_0a15e8b730ca, modelType=multinomial, numClasses=2, numFeatures=16 \nScore: 91.98 [  612 ms]\nOut[85]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">NaiveBayesModel: uid=NaiveBayes_0a15e8b730ca, modelType=multinomial, numClasses=2, numFeatures=16 \nScore: 91.98 [  612 ms]\nOut[85]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Factorization Machines (FM)\nfrom pyspark.ml.classification import FMClassifier\n\nestimador = FMClassifier(\n  stepSize=0.001,\n  labelCol=\"real\",\n  featuresCol=\"features\",\n  predictionCol=\"previsto\"\n)\n\nparams = ParamGridBuilder() \\\n    .addGrid(estimador.stepSize, [0.001, 0.01, 0.1, 1.0]) \\\n    .build()\n#fine_tune_model(estimador, params, avaliador, dados_treino, dados_teste)\n\n# só classificação binária!\nevaluate_model('FM', estimador, avaliador, dados_treino, dados_teste)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c5d6d0c-dc84-4789-86d9-e31bbced6ebc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">FMClassificationModel: uid=FMClassifier_4bb846af56db, numClasses=2, numFeatures=16, factorSize=8, fitLinear=true, fitIntercept=true \nScore: 95.03 [33890 ms]\nOut[86]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">FMClassificationModel: uid=FMClassifier_4bb846af56db, numClasses=2, numFeatures=16, factorSize=8, fitLinear=true, fitIntercept=true \nScore: 95.03 [33890 ms]\nOut[86]: DataFrame[nome: string, real: bigint, features: vector, rawPrediction: vector, probability: vector, previsto: double]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"931f6863-d33c-48fa-acfd-74d9d9966166"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#results = []\n#for key, value in models.items():\n#  tup = (key,) + value\n#  results.append(tup)\n#results"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2986f11-19d9-4dc9-947b-92e4eeeef713"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["'''from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\nschema = StructType([ \\\n  StructField(\"Model\", StringType(), True), \\\n  StructField(\"Estimator\", StringType(), True), \\\n  StructField(\"Score\", FloatType(), True), \\\n  StructField(\"Time (ms)\", IntegerType(), True) \\\n])'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb5b8347-9ab1-494e-ba69-7c8b3deae11e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[106]: &#39;from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\\nschema = StructType([   StructField(&#34;Model&#34;, StringType(), True),   StructField(&#34;Estimator&#34;, StringType(), True),   StructField(&#34;Score&#34;, FloatType(), True),   StructField(&#34;Time (ms)&#34;, IntegerType(), True) ])&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[106]: &#39;from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\\nschema = StructType([   StructField(&#34;Model&#34;, StringType(), True),   StructField(&#34;Estimator&#34;, StringType(), True),   StructField(&#34;Score&#34;, FloatType(), True),   StructField(&#34;Time (ms)&#34;, IntegerType(), True) ])&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["'''results_df = spark.createDataFrame(data=results, schema=schema)\nresults_df.printSchema()\nresults_df.show(truncate=False)'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95725d53-0156-420e-81d4-56a10ed88dd2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[107]: &#39;results_df = spark.createDataFrame(data=results, schema=schema)\\nresults_df.printSchema()\\nresults_df.show(truncate=False)&#39;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[107]: &#39;results_df = spark.createDataFrame(data=results, schema=schema)\\nresults_df.printSchema()\\nresults_df.show(truncate=False)&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82649570-14bf-487e-a1b7-7b1650db74b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["names = []\nestimators = []\nscores = []\ntimes = []\n\nfor key, value in models.items():\n  (model, score, elapsed) = value\n  names.append(key)\n  estimators.append(model)\n  scores.append(score)\n  times.append(elapsed)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3abd3426-5300-4fdd-9f13-9d209aa82827"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\nresults_df = pd.DataFrame({\n  'Model': names,\n  'Score': scores,\n  'Time (ms)': times,\n  'Estimator': estimators})\n\nresults_df.sort_values(by='Score', ascending=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66c07516-466e-4b7b-88ac-36b2af98c939"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[109]: </div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[109]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Score</th>\n      <th>Time (ms)</th>\n      <th>Estimator</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>99.615126</td>\n      <td>2393</td>\n      <td>DecisionTreeClassificationModel: uid=DecisionT...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RF</td>\n      <td>99.190351</td>\n      <td>22776</td>\n      <td>RandomForestClassificationModel: uid=RandomFor...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GB</td>\n      <td>98.934773</td>\n      <td>18598</td>\n      <td>GBTClassificationModel: uid = GBTClassifier_b2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LSVM</td>\n      <td>95.275978</td>\n      <td>9664</td>\n      <td>LinearSVCModel: uid=LinearSVC_6fc815743a5f, nu...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>FM</td>\n      <td>95.027094</td>\n      <td>33890</td>\n      <td>FMClassificationModel: uid=FMClassifier_4bb846...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LR</td>\n      <td>94.949332</td>\n      <td>1669</td>\n      <td>LogisticRegressionModel: uid=LogisticRegressio...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NB</td>\n      <td>91.981002</td>\n      <td>612</td>\n      <td>NaiveBayesModel: uid=NaiveBayes_0a15e8b730ca, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Score</th>\n      <th>Time (ms)</th>\n      <th>Estimator</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>99.615126</td>\n      <td>2393</td>\n      <td>DecisionTreeClassificationModel: uid=DecisionT...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>RF</td>\n      <td>99.190351</td>\n      <td>22776</td>\n      <td>RandomForestClassificationModel: uid=RandomFor...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GB</td>\n      <td>98.934773</td>\n      <td>18598</td>\n      <td>GBTClassificationModel: uid = GBTClassifier_b2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LSVM</td>\n      <td>95.275978</td>\n      <td>9664</td>\n      <td>LinearSVCModel: uid=LinearSVC_6fc815743a5f, nu...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>FM</td>\n      <td>95.027094</td>\n      <td>33890</td>\n      <td>FMClassificationModel: uid=FMClassifier_4bb846...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LR</td>\n      <td>94.949332</td>\n      <td>1669</td>\n      <td>LogisticRegressionModel: uid=LogisticRegressio...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NB</td>\n      <td>91.981002</td>\n      <td>612</td>\n      <td>NaiveBayesModel: uid=NaiveBayes_0a15e8b730ca, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["modelo = models['DT'][0]\nmodelo"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2774e250-cc60-4824-8cad-dc94031e423e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[94]: DecisionTreeClassificationModel: uid=DecisionTreeClassifier_fcc58529b73c, depth=9, numNodes=199, numClasses=2, numFeatures=16</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[94]: DecisionTreeClassificationModel: uid=DecisionTreeClassifier_fcc58529b73c, depth=9, numNodes=199, numClasses=2, numFeatures=16</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# gerar previsões e mostrar exemplos\nprevisoes = modelo.transform(dados_teste)\nprevisoes.select(\"nome\", \"real\", \"previsto\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ca66056-8792-4dcd-8d90-b8decee1e6fb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+----+--------+\n|        nome|real|previsto|\n+------------+----+--------+\n|    AARONSON|   1|     1.0|\n|      ABADIO|   1|     1.0|\n|      ABAITE|   0|     0.0|\n|        ABDA|   0|     0.0|\n|    ABDALLAH|   0|     0.0|\n|   ABDEILDES|   1|     1.0|\n|ABDELMOUBINE|   0|     0.0|\n|      ABDIAS|   1|     1.0|\n|   ABDINARDO|   1|     1.0|\n|    ABDNAGOS|   1|     1.0|\n+------------+----+--------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+----+--------+\n        nome|real|previsto|\n+------------+----+--------+\n    AARONSON|   1|     1.0|\n      ABADIO|   1|     1.0|\n      ABAITE|   0|     0.0|\n        ABDA|   0|     0.0|\n    ABDALLAH|   0|     0.0|\n   ABDEILDES|   1|     1.0|\nABDELMOUBINE|   0|     0.0|\n      ABDIAS|   1|     1.0|\n   ABDINARDO|   1|     1.0|\n    ABDNAGOS|   1|     1.0|\n+------------+----+--------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(previsoes.head(5))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"340e0a62-1c86-497c-942e-e70267d2ed94"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["AARONSON",1,{"vectorType":"dense","length":16,"values":[14.0,15.0,19.0,14.0,15.0,18.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]},{"vectorType":"dense","length":2,"values":[0.0,2236.0]},{"vectorType":"dense","length":2,"values":[0.0,1.0]},1.0],["ABADIO",1,{"vectorType":"dense","length":16,"values":[15.0,9.0,4.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]},{"vectorType":"dense","length":2,"values":[1.0,5894.0]},{"vectorType":"dense","length":2,"values":[1.6963528413910093E-4,0.999830364715861]},1.0],["ABAITE",0,{"vectorType":"dense","length":16,"values":[5.0,20.0,9.0,1.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]},{"vectorType":"dense","length":2,"values":[746.0,8.0]},{"vectorType":"dense","length":2,"values":[0.9893899204244032,0.010610079575596816]},0.0],["ABDA",0,{"vectorType":"dense","length":16,"values":[1.0,4.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]},{"vectorType":"dense","length":2,"values":[7789.0,0.0]},{"vectorType":"dense","length":2,"values":[1.0,0.0]},0.0],["ABDALLAH",0,{"vectorType":"dense","length":16,"values":[8.0,1.0,12.0,12.0,1.0,4.0,2.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]},{"vectorType":"dense","length":2,"values":[72.0,0.0]},{"vectorType":"dense","length":2,"values":[1.0,0.0]},0.0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"nome","type":"\"string\"","metadata":"{}"},{"name":"real","type":"\"long\"","metadata":"{}"},{"name":"features","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{}"},{"name":"rawPrediction","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{}"},{"name":"probability","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{}"},{"name":"previsto","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>nome</th><th>real</th><th>features</th><th>rawPrediction</th><th>probability</th><th>previsto</th></tr></thead><tbody><tr><td>AARONSON</td><td>1</td><td>Map(vectorType -> dense, length -> 16, values -> List(14.0, 15.0, 19.0, 14.0, 15.0, 18.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.0, 2236.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.0, 1.0))</td><td>1.0</td></tr><tr><td>ABADIO</td><td>1</td><td>Map(vectorType -> dense, length -> 16, values -> List(15.0, 9.0, 4.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(1.0, 5894.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(1.6963528413910093E-4, 0.999830364715861))</td><td>1.0</td></tr><tr><td>ABAITE</td><td>0</td><td>Map(vectorType -> dense, length -> 16, values -> List(5.0, 20.0, 9.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(746.0, 8.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(0.9893899204244032, 0.010610079575596816))</td><td>0.0</td></tr><tr><td>ABDA</td><td>0</td><td>Map(vectorType -> dense, length -> 16, values -> List(1.0, 4.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(7789.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(1.0, 0.0))</td><td>0.0</td></tr><tr><td>ABDALLAH</td><td>0</td><td>Map(vectorType -> dense, length -> 16, values -> List(8.0, 1.0, 12.0, 12.0, 1.0, 4.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(72.0, 0.0))</td><td>Map(vectorType -> dense, length -> 2, values -> List(1.0, 0.0))</td><td>0.0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# matriz de confusão\nprevisoes.groupBy(\"real\", \"previsto\").count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb29be9d-64c3-43db-95bb-898a452b92d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+--------+-----+\n|real|previsto|count|\n+----+--------+-----+\n|   1|     0.0|   23|\n|   0|     1.0|   24|\n|   0|     0.0| 6538|\n|   1|     1.0| 5670|\n+----+--------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------+-----+\nreal|previsto|count|\n+----+--------+-----+\n   1|     0.0|   23|\n   0|     1.0|   24|\n   0|     0.0| 6538|\n   1|     1.0| 5670|\n+----+--------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["previsoes.filter(\"real = 1\").filter(\"real != previsto\").select(\"nome\", \"real\", \"previsto\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1add0c85-a72b-4ab7-937a-40da67b70e42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+----+--------+\n|   nome|real|previsto|\n+-------+----+--------+\n| ADONAI|   1|     0.0|\n|   ALIX|   1|     0.0|\n|  BRUCE|   1|     0.0|\n| DESAIX|   1|     0.0|\n|    ELI|   1|     0.0|\n|    ELY|   1|     0.0|\n| ESAULL|   1|     0.0|\n| GEORGE|   1|     0.0|\n|  HERLY|   1|     0.0|\n|HERNANI|   1|     0.0|\n+-------+----+--------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----+--------+\n   nome|real|previsto|\n+-------+----+--------+\n ADONAI|   1|     0.0|\n   ALIX|   1|     0.0|\n  BRUCE|   1|     0.0|\n DESAIX|   1|     0.0|\n    ELI|   1|     0.0|\n    ELY|   1|     0.0|\n ESAULL|   1|     0.0|\n GEORGE|   1|     0.0|\n  HERLY|   1|     0.0|\nHERNANI|   1|     0.0|\n+-------+----+--------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["previsoes.filter(\"real = 0\").filter(\"real != previsto\").select(\"nome\", \"real\", \"previsto\").show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98351ffd-5768-46f8-9705-987635ee6f42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+----+--------+\n|   nome|real|previsto|\n+-------+----+--------+\n|   ANNY|   0|     1.0|\n|  DORIS|   0|     1.0|\n| ELENIR|   0|     1.0|\n| GLADIS|   0|     1.0|\n| GLEDSE|   0|     1.0|\n| IASMIM|   0|     1.0|\n| ISABEL|   0|     1.0|\n|   LADY|   0|     1.0|\n| LEONOR|   0|     1.0|\n|LILLIAN|   0|     1.0|\n+-------+----+--------+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----+--------+\n   nome|real|previsto|\n+-------+----+--------+\n   ANNY|   0|     1.0|\n  DORIS|   0|     1.0|\n ELENIR|   0|     1.0|\n GLADIS|   0|     1.0|\n GLEDSE|   0|     1.0|\n IASMIM|   0|     1.0|\n ISABEL|   0|     1.0|\n   LADY|   0|     1.0|\n LEONOR|   0|     1.0|\nLILLIAN|   0|     1.0|\n+-------+----+--------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76b807c2-a225-4eb2-b51f-3c23a7acccc3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# https://docs.databricks.com/applications/machine-learning/model-export/mleap-model-export.html#export-and-import-models-in-python\n#!pip install mleap"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fea9175f-0ac4-4094-98ca-acfe6fe57de8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\nrm -rf /tmp/mleap_python_model_export\nmkdir /tmp/mleap_python_model_export"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3463470-0502-4ec9-9219-bfe904853adb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import mleap.pyspark\nfrom mleap.pyspark.spark_support import SimpleSparkSerializer\n\nmodelo.serializeToBundle(\"jar:file:/tmp/mleap_python_model_export/72-modelo-genero-nome-pyspark-json.zip\", previsoes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9054d370-1c14-423c-9500-e3843a11439d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-723065229944245&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">from</span> mleap<span class=\"ansi-blue-fg\">.</span>pyspark<span class=\"ansi-blue-fg\">.</span>spark_support <span class=\"ansi-green-fg\">import</span> SimpleSparkSerializer\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>modelo<span class=\"ansi-blue-fg\">.</span>serializeToBundle<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;jar:file:/tmp/mleap_python_model_export/72-modelo-genero-nome-pyspark-json.zip&#34;</span><span class=\"ansi-blue-fg\">,</span> previsoes<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/mleap/pyspark/spark_support.py</span> in <span class=\"ansi-cyan-fg\">serializeToBundle</span><span class=\"ansi-blue-fg\">(self, path, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     22</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> <span class=\"ansi-green-fg\">def</span> serializeToBundle<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 24</span><span class=\"ansi-red-fg\">     </span>serializer <span class=\"ansi-blue-fg\">=</span> SimpleSparkSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     25</span>     serializer<span class=\"ansi-blue-fg\">.</span>serializeToBundle<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">=</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     26</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/mleap/pyspark/spark_support.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     37</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     38</span>         super<span class=\"ansi-blue-fg\">(</span>SimpleSparkSerializer<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 39</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_java_obj <span class=\"ansi-blue-fg\">=</span> _jvm<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>ml<span class=\"ansi-blue-fg\">.</span>combust<span class=\"ansi-blue-fg\">.</span>mleap<span class=\"ansi-blue-fg\">.</span>spark<span class=\"ansi-blue-fg\">.</span>SimpleSparkSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>     <span class=\"ansi-green-fg\">def</span> serializeToBundle<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> transformer<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: &#39;JavaPackage&#39; object is not callable</div>","errorSummary":"<span class=\"ansi-red-fg\">TypeError</span>: &#39;JavaPackage&#39; object is not callable","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-723065229944245&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">from</span> mleap<span class=\"ansi-blue-fg\">.</span>pyspark<span class=\"ansi-blue-fg\">.</span>spark_support <span class=\"ansi-green-fg\">import</span> SimpleSparkSerializer\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>modelo<span class=\"ansi-blue-fg\">.</span>serializeToBundle<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;jar:file:/tmp/mleap_python_model_export/72-modelo-genero-nome-pyspark-json.zip&#34;</span><span class=\"ansi-blue-fg\">,</span> previsoes<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/mleap/pyspark/spark_support.py</span> in <span class=\"ansi-cyan-fg\">serializeToBundle</span><span class=\"ansi-blue-fg\">(self, path, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     22</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> <span class=\"ansi-green-fg\">def</span> serializeToBundle<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 24</span><span class=\"ansi-red-fg\">     </span>serializer <span class=\"ansi-blue-fg\">=</span> SimpleSparkSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     25</span>     serializer<span class=\"ansi-blue-fg\">.</span>serializeToBundle<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">=</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     26</span> \n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.8/site-packages/mleap/pyspark/spark_support.py</span> in <span class=\"ansi-cyan-fg\">__init__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     37</span>     <span class=\"ansi-green-fg\">def</span> __init__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     38</span>         super<span class=\"ansi-blue-fg\">(</span>SimpleSparkSerializer<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>__init__<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 39</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_java_obj <span class=\"ansi-blue-fg\">=</span> _jvm<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>ml<span class=\"ansi-blue-fg\">.</span>combust<span class=\"ansi-blue-fg\">.</span>mleap<span class=\"ansi-blue-fg\">.</span>spark<span class=\"ansi-blue-fg\">.</span>SimpleSparkSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     40</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     41</span>     <span class=\"ansi-green-fg\">def</span> serializeToBundle<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> transformer<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: &#39;JavaPackage&#39; object is not callable</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3af2e16b-d9b6-4d0d-aacc-74813ddc5130"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dados = pd.DataFrame({'REAL': y, 'PREV': y_pred}, index=X.index)\nfor col in dados.columns:\n    dados[col] = dados[col].map({0: 'X', 1: 'F', 2: 'M'})\ndados.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e10cb77-76eb-413e-8f01-e082a16df114"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n\naccuracy_score(y, y_pred)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a62b636b-bb04-4bef-a68a-500741875593"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y, y_pred)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cab77263-acea-486f-9266-a33dbf9b9d36"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dados[dados['REAL'] != dados['PREV']].head(20)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"597677a2-152d-467e-8190-32a25638c5e4"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"72-modelo-genero-nome-pyspark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1866394035798127}},"nbformat":4,"nbformat_minor":0}
